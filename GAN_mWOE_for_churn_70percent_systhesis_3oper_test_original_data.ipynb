{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e09KUs74lXKH"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY90DPQfldup",
        "outputId": "b6bb70ed-243d-4fc8-c99c-30935ff34303"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVfF-CnHldxw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "bd53bb14-909e-4771-bac0-662d117cec0d"
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('drive/My Drive/Telecom_customer churn (100000).csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9e3f4f1e4958>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/My Drive/Telecom_customer churn (100000).csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/My Drive/Telecom_customer churn (100000).csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoTIexMGld0T"
      },
      "source": [
        "num_df = df.select_dtypes(include=['float64', 'int64']).copy()\n",
        "cat_df = df.select_dtypes(include=['object']).copy()\n",
        "\n",
        "# Categorical boolean mask\n",
        "categorical_feature_mask = cat_df.dtypes==object\n",
        "# filter categorical columns using mask and turn it into a list\n",
        "categorical_cols = cat_df.columns[categorical_feature_mask].tolist()\n",
        "\n",
        "import numpy as np\n",
        "#conData=np.log(0.00001 + 1)\n",
        "conData=0\n",
        "cat_df=cat_df.fillna(conData)\n",
        "num_df=num_df.fillna(conData)\n",
        "cat_df=cat_df.astype(str)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "cat_df[categorical_cols] = cat_df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
        "\n",
        "#cat_df[categorical_cols].head(10)\n",
        "\n",
        "change_mou=num_df['change_mou']\n",
        "change_rev=num_df['change_rev']\n",
        "num_df=num_df.drop(['change_mou'], axis=1)\n",
        "num_df=num_df.drop(['change_rev'], axis=1)\n",
        "\n",
        "num_df=num_df.drop(['Customer_ID'], axis=1)\n",
        "\n",
        "churn=num_df['churn']\n",
        "num_df=num_df.drop(['churn'], axis=1)\n",
        "\n",
        "#no convertion\n",
        "\n",
        "num_df=num_df.fillna(conData)\n",
        "\n",
        "num_df[num_df < 0]=0\n",
        "\n",
        "result_df = pd.concat([num_df, cat_df], axis=1)\n",
        "np.nan_to_num(result_df)\n",
        "\n",
        "result_df_op=result_df\n",
        "#result_df_op=result_df_op.drop(['churn'], axis=1)\n",
        "#result_df_op=np.nan_to_num(result_df_op)\n",
        "\n",
        "X=result_df_op\n",
        "#y=result_df['churn']\n",
        "y=churn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6RRYnQwld2s"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "train=pd.concat([X_train, y_train], axis=1)\n",
        "test=pd.concat([X_test, y_test], axis=1)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epGJwPLPld5E"
      },
      "source": [
        "from google.colab import files\n",
        "import os, sys\n",
        "file_path = 'drive/My Drive/Colab Notebooks/private-data-generation/'\n",
        "#sys.path.append(file_path)\n",
        "sys.path.append(os.path.abspath(file_path))\n",
        "import sys\n",
        "sys.path.insert(1,'/content/drive/My Drive/Colab Notebooks/private-data-generation/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHW66YHgld7k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "2f81e054-6de2-45e9-91cc-a247d4286812"
      },
      "source": [
        "from models import dp_wgan, pate_gan, ron_gauss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b60ce3e4ed0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdp_wgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpate_gan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mron_gauss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbNaMuQbleBt"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingRegressor\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
        "from sklearn import preprocessing\n",
        "from scipy.special import expit\n",
        "from models import dp_wgan, pate_gan, ron_gauss\n",
        "from models.Private_PGM import private_pgm\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rUa1LRmleGm"
      },
      "source": [
        "class  Class_opt:\n",
        "    \"This is a person class\"\n",
        "    categorical = False\n",
        "    model='dp-wgan'\n",
        "    target_epsilon=80\n",
        "    target_delta=0.8\n",
        "    downstream_task=\"classification\"\n",
        "    target_variable='status'\n",
        "    batch_size=64\n",
        "    micro_batch_size=8\n",
        "    clamp_lower=0.01\n",
        "    clamp_upper=0.01\n",
        "    clip_coeff=0.1\n",
        "    sigma=2.0\n",
        "    num_epochs=50\n",
        "    enable_privacy=0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "opt = Class_opt()\n",
        "opt.target_variable='churn'\n",
        "opt.categorical = True\n",
        "opt.model='dp-wgan'\n",
        "opt.target_epsilon=90\n",
        "opt.target_delta=0.1\n",
        "opt.batch_size=64\n",
        "opt.micro_batch_size=8\n",
        "opt.clamp_lower=0.01\n",
        "opt.clamp_upper=0.01\n",
        "opt.clip_coeff=0.1\n",
        "opt.sigma=0.8\n",
        "opt.num_epochs=100\n",
        "opt.enable_privacy=0\n",
        "\n",
        "opt.downstream_task=\"classification\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l699EcuKleJg"
      },
      "source": [
        "data_columns = [col for col in train.columns if col != opt.target_variable]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Urf-hAhrleL0",
        "outputId": "011c5824-5de5-4435-ad92-2ca693b14eb0"
      },
      "source": [
        "class_ratios = train[opt.target_variable].sort_values().groupby(train[opt.target_variable]).size().values/train.shape[0]\n",
        "class_ratios"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.50581429, 0.49418571])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IohJs9KleOG"
      },
      "source": [
        "X_train = np.nan_to_num(train.drop([opt.target_variable], axis=1).values)\n",
        "y_train = np.nan_to_num(train[opt.target_variable].values)\n",
        "X_test = np.nan_to_num(test.drop([opt.target_variable], axis=1).values)\n",
        "y_test = np.nan_to_num(test[opt.target_variable].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBhzyo1dleQ1"
      },
      "source": [
        "#Normalized the data\n",
        "X_train = expit(X_train)\n",
        "X_test = expit(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4CkWJApleTS"
      },
      "source": [
        "input_dim = X_train.shape[1]\n",
        "z_dim = int(input_dim / 4 + 1) if input_dim % 4 == 0 else int(input_dim / 4)\n",
        "\n",
        "Hyperparams = collections.namedtuple('Hyperarams','batch_size micro_batch_size clamp_lower clamp_upper clip_coeff sigma class_ratios lr num_epochs')\n",
        "Hyperparams.__new__.__defaults__ = (None, None, None, None, None, None, None, None, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60OqCIPtleV5",
        "outputId": "a230474b-245e-4dcf-f0e6-e2979403bcd0"
      },
      "source": [
        "conditional = (opt.downstream_task == \"classification\")\n",
        "model = dp_wgan.DP_WGAN(input_dim, z_dim, opt.target_epsilon, opt.target_delta, conditional)\n",
        "model.train(X_train, y_train, Hyperparams(batch_size=opt.batch_size, micro_batch_size=opt.micro_batch_size,\n",
        "                                              clamp_lower=opt.clamp_lower, clamp_upper=opt.clamp_upper,\n",
        "                                              clip_coeff=opt.clip_coeff, sigma=opt.sigma, class_ratios=class_ratios, lr=\n",
        "                                              5e-5, num_epochs=opt.num_epochs), private=opt.enable_privacy)\n",
        "\n",
        "torch.save(model, 'drive/My Drive/checkpoint_GAN_MWOE_churn_2-70percent.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 1 Loss D real :  0.3820572700371687 Loss D fake :  0.015844269001305013 Loss G :  0.015155301665575174 Epsilon spent :  0\n",
            "Epoch : 2 Loss D real :  0.38445937604780483 Loss D fake :  0.012447103213268675 Loss G :  0.011694101890798428 Epsilon spent :  0\n",
            "Epoch : 3 Loss D real :  0.38621509937047166 Loss D fake :  0.010009217077557721 Loss G :  0.010118060452458392 Epsilon spent :  0\n",
            "Epoch : 4 Loss D real :  0.38880828724207667 Loss D fake :  0.01004072108357695 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 5 Loss D real :  0.388861550144461 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 6 Loss D real :  0.38887544657523865 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 7 Loss D real :  0.3900990794590003 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 8 Loss D real :  0.3886763206323295 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 9 Loss D real :  0.39012158927812246 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 10 Loss D real :  0.38362880593866344 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 11 Loss D real :  0.3887628922757283 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 12 Loss D real :  0.38596764397187555 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 13 Loss D real :  0.38390903845017865 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 14 Loss D real :  0.38396991454844853 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 15 Loss D real :  0.38353959020953116 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 16 Loss D real :  0.3861083516286857 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 17 Loss D real :  0.3835317353845323 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 18 Loss D real :  0.38250985968783047 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 19 Loss D real :  0.3844183679457016 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 20 Loss D real :  0.3851299205132147 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 21 Loss D real :  0.3819815258796644 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 22 Loss D real :  0.38274038494130097 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 23 Loss D real :  0.38479926619823346 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 24 Loss D real :  0.38092663243094643 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 25 Loss D real :  0.3828938151401369 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 26 Loss D real :  0.37647374928401833 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 27 Loss D real :  0.38628413604294765 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 28 Loss D real :  0.38553945410464774 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 29 Loss D real :  0.38564103371230757 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 30 Loss D real :  0.38057764350988676 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 31 Loss D real :  0.38221668529171215 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 32 Loss D real :  0.38782006936231644 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 33 Loss D real :  0.3845690980179666 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 34 Loss D real :  0.384585543188031 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 35 Loss D real :  0.38254722092620086 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 36 Loss D real :  0.3869539492658981 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 37 Loss D real :  0.38366276873541993 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 38 Loss D real :  0.3887650083677868 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 39 Loss D real :  0.38771261789973377 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 40 Loss D real :  0.39151662452494806 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 41 Loss D real :  0.39256552654565946 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 42 Loss D real :  0.3872748301599992 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 43 Loss D real :  0.3876403247830871 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 44 Loss D real :  0.38394938536719536 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 45 Loss D real :  0.3859784785397198 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 46 Loss D real :  0.38742158086990175 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 47 Loss D real :  0.38286411830270906 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 48 Loss D real :  0.3880572042721423 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 49 Loss D real :  0.39292636419253496 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 50 Loss D real :  0.38331368599776805 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 51 Loss D real :  0.38877696550490193 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 52 Loss D real :  0.38105467349206057 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 53 Loss D real :  0.38075695077574556 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 54 Loss D real :  0.38601825707141035 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 55 Loss D real :  0.38739014428662255 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 56 Loss D real :  0.38546178860582747 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 57 Loss D real :  0.3848365868578477 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 58 Loss D real :  0.38479618498792045 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 59 Loss D real :  0.3857500677636704 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 60 Loss D real :  0.3909930593198424 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 61 Loss D real :  0.38154069948386904 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 62 Loss D real :  0.3829414557077141 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 63 Loss D real :  0.38892128360525513 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 64 Loss D real :  0.39006337408126657 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 65 Loss D real :  0.3823239166508584 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 66 Loss D real :  0.3891439937453581 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 67 Loss D real :  0.3838389923731803 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 68 Loss D real :  0.3852108435719558 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 69 Loss D real :  0.38755933578421964 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 70 Loss D real :  0.38402512334901184 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 71 Loss D real :  0.38584230310175827 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 72 Loss D real :  0.38941827896536696 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 73 Loss D real :  0.38270786258520295 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 74 Loss D real :  0.38546731329111206 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 75 Loss D real :  0.3837358401075387 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 76 Loss D real :  0.38099221589569865 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 77 Loss D real :  0.38367833484237657 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 78 Loss D real :  0.3861264771737874 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 79 Loss D real :  0.38248819948700696 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 80 Loss D real :  0.3873644430943602 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 81 Loss D real :  0.38330822697490335 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 82 Loss D real :  0.3834719977144453 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 83 Loss D real :  0.3856307597372659 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 84 Loss D real :  0.38289276690365004 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 85 Loss D real :  0.3828639903068781 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 86 Loss D real :  0.3854245174805704 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 87 Loss D real :  0.384027422255413 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 88 Loss D real :  0.3905294043015335 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 89 Loss D real :  0.3877566437234499 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 90 Loss D real :  0.3865764420747557 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 91 Loss D real :  0.3900097284567519 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 92 Loss D real :  0.38836902345247826 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 93 Loss D real :  0.38332949121847215 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 94 Loss D real :  0.3833469001120718 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 95 Loss D real :  0.38804194943923803 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 96 Loss D real :  0.388846186716682 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 97 Loss D real :  0.388656649503051 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 98 Loss D real :  0.3811535376948054 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 99 Loss D real :  0.387686355924384 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 100 Loss D real :  0.38848550572767687 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  0\n",
            "Epoch : 101 Loss D real :  0.3876940695908018 Loss D fake :  0.01 Loss G :  0.01 Epsilon spent :  inf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYve-c-pleYH"
      },
      "source": [
        "if opt.model == 'imle' or opt.model == 'dp-wgan' or opt.model == 'pate-gan':\n",
        "    syn_data = model.generate(X_train.shape[0], class_ratios)\n",
        "    X_syn, y_syn = syn_data[:, :-1], syn_data[:, -1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWbEXnPSleag",
        "outputId": "498c2e44-3531-433f-8246-8741ccbc1ba4"
      },
      "source": [
        "X_syn_df = pd.DataFrame(data=X_syn, columns=data_columns)\n",
        "y_syn_df = pd.DataFrame(data=y_syn, columns=[opt.target_variable])\n",
        "\n",
        "syn_df = pd.concat([X_syn_df, y_syn_df], axis=1)\n",
        "syn_df.to_csv(\"drive/My Drive/synthetic_data_GAN_mWOE_2_70percent_training.csv\")\n",
        "test.to_csv(\"drive/My Drive/Original_data_GAN_mWOE_2_30percent_test.csv\")\n",
        "print(\"Saved synthetic data at : \", \"drive/My Drive/synthetic_data_GAN_mWOE_2_70percent.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved synthetic data at :  drive/My Drive/synthetic_data_GAN_mWOE_2_70percent.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBCLf6ssoosN"
      },
      "source": [
        "import numpy as np\n",
        "class WOE_Encoder():\n",
        "    def __init__(self, cols=None, size=None):\n",
        "            self.cols = cols\n",
        "            self.min_samples=1\n",
        "            #self.bins=10000\n",
        "            self.bins=int(size/10)\n",
        "            self._mapping = {}\n",
        "\n",
        "    def WOE_fit(self, X, y):\n",
        "        for col in self.cols:\n",
        "            X[col]=X[col].fillna(-9999)\n",
        "            if (len(np.unique(X[col]))>100):\n",
        "                binned_x = pd.qcut(X[col], self.bins,  duplicates='drop')\n",
        "                d0 = pd.DataFrame({'x': binned_x, 'y':y})\n",
        "            else:\n",
        "                d0 = pd.DataFrame({'x': X[col], 'y': y})\n",
        "            #print (d0)\n",
        "            # Share of positive (resp. negative) labels for each category P(X=X_i | Y=1) (resp. P(X=X_i | Y=0))\n",
        "            #mapping = y.groupby(X[col]).agg(['sum', 'count']).rename({'sum': 'pos'}, axis=1)\n",
        "            mapping = y.groupby(d0[\"x\"]).agg(['sum', 'count']).rename({'sum': 'pos'}, axis=1)\n",
        "            mapping['neg'] = mapping['count'] - mapping['pos']\n",
        "            mapping[['pos', 'neg']] /= mapping[['pos', 'neg']].sum()\n",
        "            # For corner cases, defaulting to WOE = 0 (meaning no info). To avoid division by 0 we use default values.\n",
        "            undef = (mapping['count'] < self.min_samples) | (mapping['pos'] == 0) | (mapping['neg'] == 0)\n",
        "            mapping.loc[undef, ['pos', 'neg']] = -1\n",
        "            # Final step, log of ratio of probabily estimates\n",
        "            mapping['value'] = np.log((mapping['pos'] +0.0001)/ (mapping['neg']+0.0001))\n",
        "            self._mapping[col] = mapping\n",
        "\n",
        "\n",
        "        X_encoded = X.copy(deep=True)\n",
        "        for col, mapping in self._mapping.items():\n",
        "            X_encoded.loc[:, col] = X_encoded[col].fillna(-9999).map(mapping['value'])\n",
        "            X_encoded[col].fillna(0, inplace=True)\n",
        "\n",
        "        return X_encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "cMKrcrROoyLt",
        "outputId": "d1f763ac-0677-42a9-971c-293e98bd3bab"
      },
      "source": [
        "syn_df=pd.read_csv('drive/My Drive/synthetic_data_GAN_mWOE_2_70percent_training.csv')\n",
        "y_syn_df=syn_df['churn']\n",
        "X_syn_df=syn_df.drop(['churn'], axis=1)\n",
        "Obj = WOE_Encoder(cols=X_syn_df.columns, size=X_syn_df.shape[0])\n",
        "X_encoded = Obj.WOE_fit(X_syn_df, y_syn_df)\n",
        "X_encoded.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a74e726a-1834-43bd-b9df-49cdb4dd73c2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>rev_Mean</th>\n",
              "      <th>mou_Mean</th>\n",
              "      <th>totmrc_Mean</th>\n",
              "      <th>da_Mean</th>\n",
              "      <th>ovrmou_Mean</th>\n",
              "      <th>ovrrev_Mean</th>\n",
              "      <th>vceovr_Mean</th>\n",
              "      <th>datovr_Mean</th>\n",
              "      <th>roam_Mean</th>\n",
              "      <th>drop_vce_Mean</th>\n",
              "      <th>drop_dat_Mean</th>\n",
              "      <th>blck_vce_Mean</th>\n",
              "      <th>blck_dat_Mean</th>\n",
              "      <th>unan_vce_Mean</th>\n",
              "      <th>unan_dat_Mean</th>\n",
              "      <th>plcd_vce_Mean</th>\n",
              "      <th>plcd_dat_Mean</th>\n",
              "      <th>recv_vce_Mean</th>\n",
              "      <th>recv_sms_Mean</th>\n",
              "      <th>comp_vce_Mean</th>\n",
              "      <th>comp_dat_Mean</th>\n",
              "      <th>custcare_Mean</th>\n",
              "      <th>ccrndmou_Mean</th>\n",
              "      <th>cc_mou_Mean</th>\n",
              "      <th>inonemin_Mean</th>\n",
              "      <th>threeway_Mean</th>\n",
              "      <th>mou_cvce_Mean</th>\n",
              "      <th>mou_cdat_Mean</th>\n",
              "      <th>mou_rvce_Mean</th>\n",
              "      <th>owylis_vce_Mean</th>\n",
              "      <th>mouowylisv_Mean</th>\n",
              "      <th>iwylis_vce_Mean</th>\n",
              "      <th>mouiwylisv_Mean</th>\n",
              "      <th>peak_vce_Mean</th>\n",
              "      <th>peak_dat_Mean</th>\n",
              "      <th>mou_peav_Mean</th>\n",
              "      <th>mou_pead_Mean</th>\n",
              "      <th>opk_vce_Mean</th>\n",
              "      <th>opk_dat_Mean</th>\n",
              "      <th>...</th>\n",
              "      <th>avgmou</th>\n",
              "      <th>avgqty</th>\n",
              "      <th>avg3mou</th>\n",
              "      <th>avg3qty</th>\n",
              "      <th>avg3rev</th>\n",
              "      <th>avg6mou</th>\n",
              "      <th>avg6qty</th>\n",
              "      <th>avg6rev</th>\n",
              "      <th>hnd_price</th>\n",
              "      <th>phones</th>\n",
              "      <th>models</th>\n",
              "      <th>truck</th>\n",
              "      <th>rv</th>\n",
              "      <th>lor</th>\n",
              "      <th>adults</th>\n",
              "      <th>income</th>\n",
              "      <th>numbcars</th>\n",
              "      <th>forgntvl</th>\n",
              "      <th>eqpdays</th>\n",
              "      <th>new_cell</th>\n",
              "      <th>crclscod</th>\n",
              "      <th>asl_flag</th>\n",
              "      <th>prizm_social_one</th>\n",
              "      <th>area</th>\n",
              "      <th>dualband</th>\n",
              "      <th>refurb_new</th>\n",
              "      <th>hnd_webcap</th>\n",
              "      <th>ownrent</th>\n",
              "      <th>dwlltype</th>\n",
              "      <th>marital</th>\n",
              "      <th>infobase</th>\n",
              "      <th>HHstatin</th>\n",
              "      <th>dwllsize</th>\n",
              "      <th>ethnic</th>\n",
              "      <th>kid0_2</th>\n",
              "      <th>kid3_5</th>\n",
              "      <th>kid6_10</th>\n",
              "      <th>kid11_15</th>\n",
              "      <th>kid16_17</th>\n",
              "      <th>creditcd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-1.015310</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>-1.015310</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.007933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-1.015310</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-1.015310</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.015310</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.228536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-1.015310</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-1.015310</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-1.015310</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.228536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-1.015310</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.730463</td>\n",
              "      <td>0.007933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>...</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.471970</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>-0.228536</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>1.028017</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.487185</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.007933</td>\n",
              "      <td>0.244246</td>\n",
              "      <td>0.244246</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  97 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a74e726a-1834-43bd-b9df-49cdb4dd73c2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a74e726a-1834-43bd-b9df-49cdb4dd73c2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a74e726a-1834-43bd-b9df-49cdb4dd73c2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0  rev_Mean  mou_Mean  ...  kid11_15  kid16_17  creditcd\n",
              "0    0.007933 -0.228536  0.007933  ... -0.228536 -0.228536  0.007933\n",
              "1    0.007933 -0.228536  0.007933  ...  0.007933 -0.471970 -0.228536\n",
              "2    0.007933 -0.471970 -0.730463  ...  0.244246 -0.471970 -0.228536\n",
              "3    0.007933  0.007933  0.244246  ... -0.471970 -0.730463  0.007933\n",
              "4    0.007933 -0.228536 -0.228536  ...  0.007933  0.244246  0.244246\n",
              "\n",
              "[5 rows x 97 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qguBQPHAoyhi",
        "outputId": "d30a80f1-fb63-4fcf-f1dd-fbd76d3d1eb3"
      },
      "source": [
        "#Import Gaussian Naive Bayes model\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "#Create a Gaussian Classifier\n",
        "classifier = GaussianNB(priors=None, var_smoothing=6.579332246575682e-09)\n",
        "\n",
        "classifier.fit(X_encoded,y_syn)\n",
        "\n",
        "\n",
        "cv_method = RepeatedKFold(n_splits=10,\n",
        "                          n_repeats=3,\n",
        "                          random_state=999)\n",
        "\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred  = cross_val_predict(estimator = classifier, X = X_encoded, y = y_syn_df, cv = 5)\n",
        "tn, fp, fn, tp  = confusion_matrix(y_syn_df, y_pred).ravel()\n",
        "print(tn, fp, fn, tp)\n",
        "pod=tp/(tp+fn)\n",
        "\n",
        "print('pod: ',pod)\n",
        "pof=fp/(fp+tn)\n",
        "print ('pof: ',pof)\n",
        "auc_val=(1+pod-pof)/2\n",
        "print ('AUC: ',auc_val)\n",
        "\n",
        "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
        "print ('accuracy: ',accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35184 52 48 34716\n",
            "pod:  0.9986192613047981\n",
            "pof:  0.0014757634237711431\n",
            "AUC:  0.9985717489405134\n",
            "accuracy:  0.9985714285714286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "QWHBupMKoymL",
        "outputId": "e76a378a-200a-47c3-bee1-cf77f0a32fa8"
      },
      "source": [
        "train_df=pd.read_csv('drive/My Drive/Original_data_GAN_mWOE_2_30percent_test.csv')\n",
        "y_test=train_df['churn']\n",
        "X_test=train_df.drop(['churn'], axis=1)\n",
        "Obj_test = WOE_Encoder(cols=X_test.columns, size=X_test.shape[0])\n",
        "X_encoded_test = Obj_test.WOE_fit(X_test, y_test)\n",
        "X_encoded_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-72422266-5edd-4783-a8bf-d1f82d66158e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>rev_Mean</th>\n",
              "      <th>mou_Mean</th>\n",
              "      <th>totmrc_Mean</th>\n",
              "      <th>da_Mean</th>\n",
              "      <th>ovrmou_Mean</th>\n",
              "      <th>ovrrev_Mean</th>\n",
              "      <th>vceovr_Mean</th>\n",
              "      <th>datovr_Mean</th>\n",
              "      <th>roam_Mean</th>\n",
              "      <th>drop_vce_Mean</th>\n",
              "      <th>drop_dat_Mean</th>\n",
              "      <th>blck_vce_Mean</th>\n",
              "      <th>blck_dat_Mean</th>\n",
              "      <th>unan_vce_Mean</th>\n",
              "      <th>unan_dat_Mean</th>\n",
              "      <th>plcd_vce_Mean</th>\n",
              "      <th>plcd_dat_Mean</th>\n",
              "      <th>recv_vce_Mean</th>\n",
              "      <th>recv_sms_Mean</th>\n",
              "      <th>comp_vce_Mean</th>\n",
              "      <th>comp_dat_Mean</th>\n",
              "      <th>custcare_Mean</th>\n",
              "      <th>ccrndmou_Mean</th>\n",
              "      <th>cc_mou_Mean</th>\n",
              "      <th>inonemin_Mean</th>\n",
              "      <th>threeway_Mean</th>\n",
              "      <th>mou_cvce_Mean</th>\n",
              "      <th>mou_cdat_Mean</th>\n",
              "      <th>mou_rvce_Mean</th>\n",
              "      <th>owylis_vce_Mean</th>\n",
              "      <th>mouowylisv_Mean</th>\n",
              "      <th>iwylis_vce_Mean</th>\n",
              "      <th>mouiwylisv_Mean</th>\n",
              "      <th>peak_vce_Mean</th>\n",
              "      <th>peak_dat_Mean</th>\n",
              "      <th>mou_peav_Mean</th>\n",
              "      <th>mou_pead_Mean</th>\n",
              "      <th>opk_vce_Mean</th>\n",
              "      <th>opk_dat_Mean</th>\n",
              "      <th>...</th>\n",
              "      <th>avgmou</th>\n",
              "      <th>avgqty</th>\n",
              "      <th>avg3mou</th>\n",
              "      <th>avg3qty</th>\n",
              "      <th>avg3rev</th>\n",
              "      <th>avg6mou</th>\n",
              "      <th>avg6qty</th>\n",
              "      <th>avg6rev</th>\n",
              "      <th>hnd_price</th>\n",
              "      <th>phones</th>\n",
              "      <th>models</th>\n",
              "      <th>truck</th>\n",
              "      <th>rv</th>\n",
              "      <th>lor</th>\n",
              "      <th>adults</th>\n",
              "      <th>income</th>\n",
              "      <th>numbcars</th>\n",
              "      <th>forgntvl</th>\n",
              "      <th>eqpdays</th>\n",
              "      <th>new_cell</th>\n",
              "      <th>crclscod</th>\n",
              "      <th>asl_flag</th>\n",
              "      <th>prizm_social_one</th>\n",
              "      <th>area</th>\n",
              "      <th>dualband</th>\n",
              "      <th>refurb_new</th>\n",
              "      <th>hnd_webcap</th>\n",
              "      <th>ownrent</th>\n",
              "      <th>dwlltype</th>\n",
              "      <th>marital</th>\n",
              "      <th>infobase</th>\n",
              "      <th>HHstatin</th>\n",
              "      <th>dwllsize</th>\n",
              "      <th>ethnic</th>\n",
              "      <th>kid0_2</th>\n",
              "      <th>kid3_5</th>\n",
              "      <th>kid6_10</th>\n",
              "      <th>kid11_15</th>\n",
              "      <th>kid16_17</th>\n",
              "      <th>creditcd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.437683</td>\n",
              "      <td>0.438523</td>\n",
              "      <td>0.313311</td>\n",
              "      <td>-0.212614</td>\n",
              "      <td>0.023354</td>\n",
              "      <td>0.272117</td>\n",
              "      <td>0.003481</td>\n",
              "      <td>0.177944</td>\n",
              "      <td>0.012381</td>\n",
              "      <td>0.011871</td>\n",
              "      <td>0.065856</td>\n",
              "      <td>0.002696</td>\n",
              "      <td>-0.141508</td>\n",
              "      <td>0.003121</td>\n",
              "      <td>0.418318</td>\n",
              "      <td>0.002758</td>\n",
              "      <td>0.194593</td>\n",
              "      <td>0.012919</td>\n",
              "      <td>-0.033138</td>\n",
              "      <td>0.000771</td>\n",
              "      <td>-0.144748</td>\n",
              "      <td>0.011646</td>\n",
              "      <td>0.065610</td>\n",
              "      <td>0.066488</td>\n",
              "      <td>0.073968</td>\n",
              "      <td>0.233981</td>\n",
              "      <td>0.035462</td>\n",
              "      <td>0.170147</td>\n",
              "      <td>0.016879</td>\n",
              "      <td>-0.306999</td>\n",
              "      <td>-0.406773</td>\n",
              "      <td>0.639069</td>\n",
              "      <td>-0.187007</td>\n",
              "      <td>0.513857</td>\n",
              "      <td>-0.375624</td>\n",
              "      <td>0.008716</td>\n",
              "      <td>0.639069</td>\n",
              "      <td>0.009909</td>\n",
              "      <td>0.283367</td>\n",
              "      <td>0.005483</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.306999</td>\n",
              "      <td>0.313311</td>\n",
              "      <td>1.039462</td>\n",
              "      <td>-0.156376</td>\n",
              "      <td>-0.166814</td>\n",
              "      <td>0.513857</td>\n",
              "      <td>0.178306</td>\n",
              "      <td>0.069988</td>\n",
              "      <td>0.292933</td>\n",
              "      <td>0.066103</td>\n",
              "      <td>0.059352</td>\n",
              "      <td>-0.000358</td>\n",
              "      <td>-0.005735</td>\n",
              "      <td>0.050631</td>\n",
              "      <td>-0.029895</td>\n",
              "      <td>-0.072531</td>\n",
              "      <td>-0.017024</td>\n",
              "      <td>0.002224</td>\n",
              "      <td>0.617782</td>\n",
              "      <td>0.013013</td>\n",
              "      <td>0.095777</td>\n",
              "      <td>0.0474</td>\n",
              "      <td>-0.014155</td>\n",
              "      <td>0.041675</td>\n",
              "      <td>0.146899</td>\n",
              "      <td>-0.027948</td>\n",
              "      <td>0.236921</td>\n",
              "      <td>-0.036852</td>\n",
              "      <td>-0.058392</td>\n",
              "      <td>-0.024435</td>\n",
              "      <td>-0.033376</td>\n",
              "      <td>-0.045371</td>\n",
              "      <td>-0.056840</td>\n",
              "      <td>0.079190</td>\n",
              "      <td>0.004204</td>\n",
              "      <td>0.001431</td>\n",
              "      <td>0.005313</td>\n",
              "      <td>0.004207</td>\n",
              "      <td>0.005265</td>\n",
              "      <td>-0.028516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.306999</td>\n",
              "      <td>-0.163961</td>\n",
              "      <td>-0.632909</td>\n",
              "      <td>-0.301970</td>\n",
              "      <td>0.023354</td>\n",
              "      <td>0.003890</td>\n",
              "      <td>0.589992</td>\n",
              "      <td>0.537637</td>\n",
              "      <td>0.012381</td>\n",
              "      <td>0.011871</td>\n",
              "      <td>0.003404</td>\n",
              "      <td>0.002696</td>\n",
              "      <td>0.103595</td>\n",
              "      <td>0.003121</td>\n",
              "      <td>-0.087403</td>\n",
              "      <td>0.002758</td>\n",
              "      <td>0.513857</td>\n",
              "      <td>0.012919</td>\n",
              "      <td>-0.033138</td>\n",
              "      <td>0.000771</td>\n",
              "      <td>-0.966045</td>\n",
              "      <td>0.011646</td>\n",
              "      <td>-0.204008</td>\n",
              "      <td>-0.060905</td>\n",
              "      <td>-0.734299</td>\n",
              "      <td>0.100242</td>\n",
              "      <td>-0.064649</td>\n",
              "      <td>0.003179</td>\n",
              "      <td>0.016879</td>\n",
              "      <td>-0.306999</td>\n",
              "      <td>-0.145795</td>\n",
              "      <td>-0.632909</td>\n",
              "      <td>0.035347</td>\n",
              "      <td>-0.632909</td>\n",
              "      <td>0.003674</td>\n",
              "      <td>0.008716</td>\n",
              "      <td>-0.163961</td>\n",
              "      <td>0.009909</td>\n",
              "      <td>-0.543300</td>\n",
              "      <td>0.005483</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.632909</td>\n",
              "      <td>-0.995608</td>\n",
              "      <td>0.313311</td>\n",
              "      <td>-0.798904</td>\n",
              "      <td>0.036678</td>\n",
              "      <td>-0.567158</td>\n",
              "      <td>-0.538583</td>\n",
              "      <td>-0.052771</td>\n",
              "      <td>-0.343276</td>\n",
              "      <td>0.066103</td>\n",
              "      <td>0.059352</td>\n",
              "      <td>-0.000358</td>\n",
              "      <td>-0.005735</td>\n",
              "      <td>0.096934</td>\n",
              "      <td>0.111627</td>\n",
              "      <td>0.089167</td>\n",
              "      <td>0.029305</td>\n",
              "      <td>0.002224</td>\n",
              "      <td>-0.085955</td>\n",
              "      <td>0.002447</td>\n",
              "      <td>0.033759</td>\n",
              "      <td>0.0474</td>\n",
              "      <td>0.040969</td>\n",
              "      <td>-0.106260</td>\n",
              "      <td>-0.025757</td>\n",
              "      <td>-0.027948</td>\n",
              "      <td>-0.084958</td>\n",
              "      <td>0.065858</td>\n",
              "      <td>0.076933</td>\n",
              "      <td>0.088142</td>\n",
              "      <td>-0.033376</td>\n",
              "      <td>0.074457</td>\n",
              "      <td>0.071067</td>\n",
              "      <td>-0.055289</td>\n",
              "      <td>0.004204</td>\n",
              "      <td>0.001431</td>\n",
              "      <td>0.005313</td>\n",
              "      <td>0.004207</td>\n",
              "      <td>0.005265</td>\n",
              "      <td>0.075098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.003179</td>\n",
              "      <td>0.313311</td>\n",
              "      <td>0.514298</td>\n",
              "      <td>0.332905</td>\n",
              "      <td>0.023354</td>\n",
              "      <td>-0.047286</td>\n",
              "      <td>-0.041586</td>\n",
              "      <td>-0.054922</td>\n",
              "      <td>0.012381</td>\n",
              "      <td>-0.264972</td>\n",
              "      <td>-0.039666</td>\n",
              "      <td>0.002696</td>\n",
              "      <td>-0.003543</td>\n",
              "      <td>0.003121</td>\n",
              "      <td>-0.012362</td>\n",
              "      <td>0.002758</td>\n",
              "      <td>0.324752</td>\n",
              "      <td>0.012919</td>\n",
              "      <td>-0.124705</td>\n",
              "      <td>0.000771</td>\n",
              "      <td>-0.084342</td>\n",
              "      <td>0.011646</td>\n",
              "      <td>0.046233</td>\n",
              "      <td>-0.005443</td>\n",
              "      <td>-0.156709</td>\n",
              "      <td>-0.148454</td>\n",
              "      <td>0.035462</td>\n",
              "      <td>-1.432487</td>\n",
              "      <td>0.016879</td>\n",
              "      <td>0.370692</td>\n",
              "      <td>-0.061077</td>\n",
              "      <td>0.253873</td>\n",
              "      <td>0.124656</td>\n",
              "      <td>0.131346</td>\n",
              "      <td>0.230270</td>\n",
              "      <td>0.008716</td>\n",
              "      <td>-0.139858</td>\n",
              "      <td>0.009909</td>\n",
              "      <td>-0.068082</td>\n",
              "      <td>0.005483</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.139858</td>\n",
              "      <td>-0.163961</td>\n",
              "      <td>-0.372643</td>\n",
              "      <td>-0.220783</td>\n",
              "      <td>-0.186821</td>\n",
              "      <td>0.003880</td>\n",
              "      <td>0.136455</td>\n",
              "      <td>-0.059028</td>\n",
              "      <td>-0.075674</td>\n",
              "      <td>-0.088213</td>\n",
              "      <td>-0.174997</td>\n",
              "      <td>0.001590</td>\n",
              "      <td>-0.005735</td>\n",
              "      <td>-0.216129</td>\n",
              "      <td>-0.029895</td>\n",
              "      <td>0.012262</td>\n",
              "      <td>-0.047168</td>\n",
              "      <td>0.002224</td>\n",
              "      <td>-0.363980</td>\n",
              "      <td>0.002447</td>\n",
              "      <td>-0.075056</td>\n",
              "      <td>0.0474</td>\n",
              "      <td>-0.014155</td>\n",
              "      <td>-0.161642</td>\n",
              "      <td>-0.339789</td>\n",
              "      <td>-0.027948</td>\n",
              "      <td>-0.084958</td>\n",
              "      <td>-0.036852</td>\n",
              "      <td>-0.058392</td>\n",
              "      <td>-0.095783</td>\n",
              "      <td>-0.033376</td>\n",
              "      <td>0.074457</td>\n",
              "      <td>-0.056840</td>\n",
              "      <td>0.080604</td>\n",
              "      <td>0.004204</td>\n",
              "      <td>0.001431</td>\n",
              "      <td>0.005313</td>\n",
              "      <td>0.004207</td>\n",
              "      <td>0.005265</td>\n",
              "      <td>-0.028516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.313311</td>\n",
              "      <td>-0.166884</td>\n",
              "      <td>-0.139858</td>\n",
              "      <td>-0.212614</td>\n",
              "      <td>0.023354</td>\n",
              "      <td>-0.047286</td>\n",
              "      <td>-0.041586</td>\n",
              "      <td>-0.054922</td>\n",
              "      <td>0.012381</td>\n",
              "      <td>0.011871</td>\n",
              "      <td>0.084238</td>\n",
              "      <td>0.002696</td>\n",
              "      <td>0.060359</td>\n",
              "      <td>0.003121</td>\n",
              "      <td>-0.175195</td>\n",
              "      <td>0.002758</td>\n",
              "      <td>0.340302</td>\n",
              "      <td>0.012919</td>\n",
              "      <td>0.066611</td>\n",
              "      <td>0.000771</td>\n",
              "      <td>0.328146</td>\n",
              "      <td>0.011646</td>\n",
              "      <td>0.065610</td>\n",
              "      <td>0.066488</td>\n",
              "      <td>0.073968</td>\n",
              "      <td>0.043614</td>\n",
              "      <td>0.035462</td>\n",
              "      <td>0.003179</td>\n",
              "      <td>0.016879</td>\n",
              "      <td>1.101565</td>\n",
              "      <td>0.095284</td>\n",
              "      <td>0.313311</td>\n",
              "      <td>0.124656</td>\n",
              "      <td>0.131346</td>\n",
              "      <td>0.288508</td>\n",
              "      <td>0.008716</td>\n",
              "      <td>-0.507795</td>\n",
              "      <td>0.009909</td>\n",
              "      <td>0.243307</td>\n",
              "      <td>0.005483</td>\n",
              "      <td>...</td>\n",
              "      <td>1.101093</td>\n",
              "      <td>0.003179</td>\n",
              "      <td>-0.159751</td>\n",
              "      <td>-0.163015</td>\n",
              "      <td>-0.071297</td>\n",
              "      <td>0.681730</td>\n",
              "      <td>-0.086247</td>\n",
              "      <td>0.122587</td>\n",
              "      <td>0.292933</td>\n",
              "      <td>0.066103</td>\n",
              "      <td>0.059352</td>\n",
              "      <td>-0.000358</td>\n",
              "      <td>-0.005735</td>\n",
              "      <td>0.096934</td>\n",
              "      <td>-0.029523</td>\n",
              "      <td>-0.072531</td>\n",
              "      <td>0.029305</td>\n",
              "      <td>0.002224</td>\n",
              "      <td>0.545013</td>\n",
              "      <td>0.002447</td>\n",
              "      <td>0.095777</td>\n",
              "      <td>0.0474</td>\n",
              "      <td>0.040969</td>\n",
              "      <td>-0.161642</td>\n",
              "      <td>0.146899</td>\n",
              "      <td>-0.027948</td>\n",
              "      <td>0.236921</td>\n",
              "      <td>0.065858</td>\n",
              "      <td>-0.058392</td>\n",
              "      <td>0.088142</td>\n",
              "      <td>-0.033376</td>\n",
              "      <td>-0.015970</td>\n",
              "      <td>-0.056840</td>\n",
              "      <td>-0.015729</td>\n",
              "      <td>0.004204</td>\n",
              "      <td>0.001431</td>\n",
              "      <td>0.005313</td>\n",
              "      <td>0.004207</td>\n",
              "      <td>0.005265</td>\n",
              "      <td>0.075098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.306999</td>\n",
              "      <td>0.003179</td>\n",
              "      <td>0.271556</td>\n",
              "      <td>-0.632909</td>\n",
              "      <td>0.023354</td>\n",
              "      <td>0.049976</td>\n",
              "      <td>-0.727461</td>\n",
              "      <td>-0.798904</td>\n",
              "      <td>0.012381</td>\n",
              "      <td>0.011871</td>\n",
              "      <td>0.809331</td>\n",
              "      <td>0.002696</td>\n",
              "      <td>-0.704332</td>\n",
              "      <td>0.003121</td>\n",
              "      <td>-0.376160</td>\n",
              "      <td>0.002758</td>\n",
              "      <td>-0.121808</td>\n",
              "      <td>0.012919</td>\n",
              "      <td>-0.359512</td>\n",
              "      <td>0.000771</td>\n",
              "      <td>0.531772</td>\n",
              "      <td>0.011646</td>\n",
              "      <td>0.046233</td>\n",
              "      <td>-0.063367</td>\n",
              "      <td>0.302157</td>\n",
              "      <td>-0.363697</td>\n",
              "      <td>-0.050028</td>\n",
              "      <td>0.313311</td>\n",
              "      <td>0.016879</td>\n",
              "      <td>0.513857</td>\n",
              "      <td>0.503820</td>\n",
              "      <td>0.170147</td>\n",
              "      <td>-0.329857</td>\n",
              "      <td>-0.543300</td>\n",
              "      <td>0.287530</td>\n",
              "      <td>0.008716</td>\n",
              "      <td>-0.163961</td>\n",
              "      <td>0.009909</td>\n",
              "      <td>0.003758</td>\n",
              "      <td>0.005483</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.632909</td>\n",
              "      <td>-0.163961</td>\n",
              "      <td>0.003845</td>\n",
              "      <td>0.049976</td>\n",
              "      <td>0.058609</td>\n",
              "      <td>-0.235396</td>\n",
              "      <td>0.132355</td>\n",
              "      <td>-0.000775</td>\n",
              "      <td>0.292933</td>\n",
              "      <td>0.066103</td>\n",
              "      <td>0.059352</td>\n",
              "      <td>-0.000358</td>\n",
              "      <td>-0.005735</td>\n",
              "      <td>-0.031102</td>\n",
              "      <td>-0.029895</td>\n",
              "      <td>-0.033467</td>\n",
              "      <td>0.029305</td>\n",
              "      <td>-0.036672</td>\n",
              "      <td>-0.701990</td>\n",
              "      <td>0.002447</td>\n",
              "      <td>0.164669</td>\n",
              "      <td>0.0474</td>\n",
              "      <td>-0.053067</td>\n",
              "      <td>0.025368</td>\n",
              "      <td>0.146899</td>\n",
              "      <td>-0.027948</td>\n",
              "      <td>0.236921</td>\n",
              "      <td>-0.036852</td>\n",
              "      <td>-0.058392</td>\n",
              "      <td>-0.095783</td>\n",
              "      <td>-0.033376</td>\n",
              "      <td>0.025427</td>\n",
              "      <td>-0.056840</td>\n",
              "      <td>0.079190</td>\n",
              "      <td>0.004204</td>\n",
              "      <td>0.001431</td>\n",
              "      <td>0.005313</td>\n",
              "      <td>0.004207</td>\n",
              "      <td>0.005265</td>\n",
              "      <td>-0.028516</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  97 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72422266-5edd-4783-a8bf-d1f82d66158e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-72422266-5edd-4783-a8bf-d1f82d66158e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-72422266-5edd-4783-a8bf-d1f82d66158e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0  rev_Mean  mou_Mean  ...  kid11_15  kid16_17  creditcd\n",
              "0    1.437683  0.438523  0.313311  ...  0.004207  0.005265 -0.028516\n",
              "1   -0.306999 -0.163961 -0.632909  ...  0.004207  0.005265  0.075098\n",
              "2    0.003179  0.313311  0.514298  ...  0.004207  0.005265 -0.028516\n",
              "3    0.313311 -0.166884 -0.139858  ...  0.004207  0.005265  0.075098\n",
              "4   -0.306999  0.003179  0.271556  ...  0.004207  0.005265 -0.028516\n",
              "\n",
              "[5 rows x 97 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMItQgYFrczl",
        "outputId": "9a99fb30-c52c-40d0-ef9b-3cc5dbf8d349"
      },
      "source": [
        "#Import Gaussian Naive Bayes model\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "#Create a Gaussian Classifier\n",
        "classifier = GaussianNB(priors=None, var_smoothing=6.579332246575682e-09)\n",
        "\n",
        "classifier.fit(X_encoded,y_syn_df)\n",
        "\n",
        "\n",
        "cv_method = RepeatedKFold(n_splits=10,\n",
        "                          n_repeats=3,\n",
        "                          random_state=999)\n",
        "\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#y_pred  = cross_val_predict(estimator = classifier, X = X_encoded, y = y_syn, cv = 5)\n",
        "y_pred = classifier.predict(X_encoded_test)\n",
        "tn, fp, fn, tp  = confusion_matrix(y_test, y_pred).ravel()\n",
        "print(tn, fp, fn, tp)\n",
        "pod=tp/(tp+fn)\n",
        "\n",
        "print('pod: ',pod)\n",
        "pof=fp/(fp+tn)\n",
        "print ('pof: ',pof)\n",
        "auc_val=(1+pod-pof)/2\n",
        "print ('AUC: ',auc_val)\n",
        "\n",
        "recall = tp/(tp+fn)\n",
        "print ('recall: ',recall)\n",
        "precision=tp/(tp+fp)\n",
        "print ('precision: ',precision)\n",
        "\n",
        "F1_score=(2*precision*recall)/(precision+recall)\n",
        "print ('F1-Score: ',F1_score)\n",
        "\n",
        "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
        "print ('accuracy: ',accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12591 2440 1550 13419\n",
            "pod:  0.8964526688489545\n",
            "pof:  0.16233118222340495\n",
            "AUC:  0.8670607433127747\n",
            "recall:  0.8964526688489545\n",
            "precision:  0.8461441452802825\n",
            "F1-Score:  0.8705722070844687\n",
            "accuracy:  0.867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOrH5jYuqKY-",
        "outputId": "09d3ad1b-6f6b-4f7e-f855-151366541fff"
      },
      "source": [
        "#LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "classifier = LogisticRegression(C=5, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
        "          n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
        "          tol=0.0001, verbose=0, warm_start=False)\n",
        "classifier.fit(X_encoded,y_syn_df)\n",
        "\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#y_pred  = cross_val_predict(estimator = classifier, X = X_encoded, y = y_syn, cv = 10)\n",
        "y_pred = classifier.predict(X_encoded_test)\n",
        "tn, fp, fn, tp  = confusion_matrix(y_test, y_pred).ravel()\n",
        "print(tn, fp, fn, tp)\n",
        "pod=tp/(tp+fn)\n",
        "\n",
        "print('pod: ',pod)\n",
        "pof=fp/(fp+tn)\n",
        "print ('pof: ',pof)\n",
        "auc_val=(1+pod-pof)/2\n",
        "print ('AUC: ',auc_val)\n",
        "\n",
        "recall = tp/(tp+fn)\n",
        "print ('recall: ',recall)\n",
        "precision=tp/(tp+fp)\n",
        "print ('precision: ',precision)\n",
        "\n",
        "F1_score=(2*precision*recall)/(precision+recall)\n",
        "print ('F1-Score: ',F1_score)\n",
        "\n",
        "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
        "print ('accuracy: ',accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12485 2546 1811 13158\n",
            "pod:  0.8790166343777139\n",
            "pof:  0.16938327456589714\n",
            "AUC:  0.8548166799059084\n",
            "recall:  0.8790166343777139\n",
            "precision:  0.8378757004584819\n",
            "F1-Score:  0.857953248785577\n",
            "accuracy:  0.8547666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDBOguZNoypv",
        "outputId": "5028a7cc-3b74-4049-c550-69f79f247276"
      },
      "source": [
        "#Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier =RandomForestClassifier(bootstrap= True, max_depth= 80, max_features= 3, min_samples_leaf= 3, min_samples_split= 12, n_estimators= 1000)\n",
        "classifier.fit(X_encoded,y_syn_df)\n",
        "\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
        "y_pred = classifier.predict(X_encoded_test)\n",
        "tn, fp, fn, tp  = confusion_matrix(y_test, y_pred).ravel()\n",
        "print(tn, fp, fn, tp)\n",
        "pod=tp/(tp+fn)\n",
        "\n",
        "print('pod: ',pod)\n",
        "pof=fp/(fp+tn)\n",
        "print ('pof: ',pof)\n",
        "auc_val=(1+pod-pof)/2\n",
        "print ('AUC: ',auc_val)\n",
        "\n",
        "recall = tp/(tp+fn)\n",
        "print ('recall: ',recall)\n",
        "precision=tp/(tp+fp)\n",
        "print ('precision: ',precision)\n",
        "\n",
        "F1_score=(2*precision*recall)/(precision+recall)\n",
        "print ('F1-Score: ',F1_score)\n",
        "\n",
        "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
        "print ('accuracy: ',accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13591 1440 7021 7948\n",
            "pod:  0.5309639922506514\n",
            "pof:  0.09580200918102588\n",
            "AUC:  0.7175809915348127\n",
            "recall:  0.5309639922506514\n",
            "precision:  0.8466126970600767\n",
            "F1-Score:  0.6526255285954757\n",
            "accuracy:  0.7179666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEQyU6GYs-sX",
        "outputId": "2fdedc4d-6358-45a5-d0f4-f1bac7758319"
      },
      "source": [
        "# KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(metric='manhattan', weights='uniform', n_neighbors=19 )\n",
        "classifier.fit(X_encoded,y_syn_df)\n",
        "\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
        "y_pred = classifier.predict(X_encoded_test)\n",
        "tn, fp, fn, tp  = confusion_matrix(y_test, y_pred).ravel()\n",
        "print(tn, fp, fn, tp)\n",
        "pod=tp/(tp+fn)\n",
        "\n",
        "print('pod: ',pod)\n",
        "pof=fp/(fp+tn)\n",
        "print ('pof: ',pof)\n",
        "auc_val=(1+pod-pof)/2\n",
        "print ('AUC: ',auc_val)\n",
        "\n",
        "recall = tp/(tp+fn)\n",
        "print ('recall: ',recall)\n",
        "precision=tp/(tp+fp)\n",
        "print ('precision: ',precision)\n",
        "\n",
        "F1_score=(2*precision*recall)/(precision+recall)\n",
        "print ('F1-Score: ',F1_score)\n",
        "\n",
        "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
        "print ('accuracy: ',accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12130 2901 1789 13180\n",
            "pod:  0.880486338432761\n",
            "pof:  0.1930011309959417\n",
            "AUC:  0.8437426037184097\n",
            "recall:  0.880486338432761\n",
            "precision:  0.8196007710963249\n",
            "F1-Score:  0.8489533011272142\n",
            "accuracy:  0.8436666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OJcphDubMR-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMUhwuhl6xSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98069651-4523-4e4d-9af0-de8df4cd69dd"
      },
      "source": [
        "#Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "#classifier = DecisionTreeClassifier(criterion='entropy', splitter='best')\n",
        "classifier = DecisionTreeClassifier()\n",
        "classifier.fit(X_encoded,y_syn_df)\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
        "y_pred = classifier.predict(X_encoded_test)\n",
        "tn, fp, fn, tp  = confusion_matrix(y_test, y_pred).ravel()\n",
        "print(tn, fp, fn, tp)\n",
        "pod=tp/(tp+fn)\n",
        "\n",
        "print('pod: ',pod)\n",
        "pof=fp/(fp+tn)\n",
        "print ('pof: ',pof)\n",
        "auc_val=(1+pod-pof)/2\n",
        "print ('AUC: ',auc_val)\n",
        "\n",
        "\n",
        "recall = tp/(tp+fn)\n",
        "print ('recall: ',recall)\n",
        "precision=tp/(tp+fp)\n",
        "print ('precision: ',precision)\n",
        "\n",
        "F1_score=(2*precision*recall)/(precision+recall)\n",
        "print ('F1-Score: ',F1_score)\n",
        "\n",
        "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
        "print ('accuracy: ',accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11499 3532 8505 6464\n",
            "pod:  0.43182577326474714\n",
            "pof:  0.23498103918568292\n",
            "AUC:  0.5984223670395321\n",
            "recall:  0.43182577326474714\n",
            "precision:  0.6466586634653861\n",
            "F1-Score:  0.5178449829761665\n",
            "accuracy:  0.5987666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Gradient Boosting\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "#classifier = GradientBoostingClassifier(max_features=None, max_depth=3, criterion='friedman_mse')\n",
        "classifier = GradientBoostingClassifier()\n",
        "classifier.fit(X_encoded,y_syn_df)\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred = classifier.predict(X_encoded_test)\n",
        "tn, fp, fn, tp  = confusion_matrix(y_test, y_pred).ravel()\n",
        "print(tn, fp, fn, tp)\n",
        "pod=tp/(tp+fn)\n",
        "\n",
        "print('pod: ',pod)\n",
        "pof=fp/(fp+tn)\n",
        "print ('pof: ',pof)\n",
        "auc_val=(1+pod-pof)/2\n",
        "print ('AUC: ',auc_val)\n",
        "\n",
        "\n",
        "recall = tp/(tp+fn)\n",
        "print ('recall: ',recall)\n",
        "precision=tp/(tp+fp)\n",
        "print ('precision: ',precision)\n",
        "\n",
        "F1_score=(2*precision*recall)/(precision+recall)\n",
        "print ('F1-Score: ',F1_score)\n",
        "\n",
        "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
        "print ('accuracy: ',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH3-0bPkKhS_",
        "outputId": "3a3fd60d-61a4-48f4-9b71-e07c80345783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14328 703 10807 4162\n",
            "pod:  0.2780412853230009\n",
            "pof:  0.046770008648792494\n",
            "AUC:  0.6156356383371041\n",
            "recall:  0.2780412853230009\n",
            "precision:  0.8554984583761562\n",
            "F1-Score:  0.41968337198749617\n",
            "accuracy:  0.6163333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#XGBClassifier\n",
        "from xgboost import XGBClassifier\n",
        "classifier =  XGBClassifier(random_state=0)\n",
        "\n",
        "classifier.fit(X_encoded,y_syn_df)\n",
        "\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#y_pred  = cross_val_predict(estimator = classifier, X = X, y = y, cv = 10)\n",
        "y_pred = classifier.predict(X_encoded_test)\n",
        "tn, fp, fn, tp  = confusion_matrix(y_test, y_pred).ravel()\n",
        "print(tn, fp, fn, tp)\n",
        "pod=tp/(tp+fn)\n",
        "\n",
        "print('pod: ',pod)\n",
        "pof=fp/(fp+tn)\n",
        "print ('pof: ',pof)\n",
        "auc_val=(1+pod-pof)/2\n",
        "print ('AUC: ',auc_val)\n",
        "\n",
        "\n",
        "recall = tp/(tp+fn)\n",
        "print ('recall: ',recall)\n",
        "precision=tp/(tp+fp)\n",
        "print ('precision: ',precision)\n",
        "\n",
        "F1_score=(2*precision*recall)/(precision+recall)\n",
        "print ('F1-Score: ',F1_score)\n",
        "\n",
        "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
        "print ('accuracy: ',accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ8yF9sXKwnE",
        "outputId": "9e86a0d6-7bd6-41b1-f01e-9de3023788f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14022 1009 10346 4623\n",
            "pod:  0.308838265749215\n",
            "pof:  0.06712793559976049\n",
            "AUC:  0.6208551650747273\n",
            "recall:  0.308838265749215\n",
            "precision:  0.8208451704545454\n",
            "F1-Score:  0.44881316440949465\n",
            "accuracy:  0.6215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FNN\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "#create model\n",
        "model = Sequential()\n",
        "#get number of columns in training data\n",
        "n_cols = X_encoded_test.shape[1]\n",
        "model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "#compile model using mse as a measure of model performance\n",
        "#model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "from keras.callbacks import EarlyStopping\n",
        "#set early stopping monitor so the model stops training when it won't improve anymore\n",
        "early_stopping_monitor = EarlyStopping(patience=3)\n",
        "#train model\n",
        "\n",
        "model.fit(X_encoded, y_syn_df, validation_split=0.2, epochs=30, callbacks=[early_stopping_monitor])\n",
        "y_pred = model.predict(X_encoded_test)\n",
        "y22_pred=y_pred.round()\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "tn, fp, fn, tp  = confusion_matrix(y_test, y22_pred).ravel()\n",
        "print(tn, fp, fn, tp)\n",
        "pod=tp/(tp+fn)\n",
        "\n",
        "print('pod: ',pod)\n",
        "pof=fp/(fp+tn)\n",
        "print ('pof: ',pof)\n",
        "auc_val=(1+pod-pof)/2\n",
        "print ('AUC: ',auc_val)\n",
        "\n",
        "\n",
        "recall = tp/(tp+fn)\n",
        "print ('recall: ',recall)\n",
        "precision=tp/(tp+fp)\n",
        "print ('precision: ',precision)\n",
        "\n",
        "F1_score=(2*precision*recall)/(precision+recall)\n",
        "print ('F1-Score: ',F1_score)\n",
        "\n",
        "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
        "print ('accuracy: ',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLlGnzw8MTae",
        "outputId": "89f598a6-6ca8-4737-a3da-ed426aa9b0ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1750/1750 [==============================] - 8s 5ms/step - loss: 0.0187 - accuracy: 0.9937 - val_loss: 0.0073 - val_accuracy: 0.9977\n",
            "Epoch 2/30\n",
            "1750/1750 [==============================] - 8s 4ms/step - loss: 0.0043 - accuracy: 0.9986 - val_loss: 0.0059 - val_accuracy: 0.9984\n",
            "Epoch 3/30\n",
            "1750/1750 [==============================] - 8s 4ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.0069 - val_accuracy: 0.9979\n",
            "Epoch 4/30\n",
            "1750/1750 [==============================] - 7s 4ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.0100 - val_accuracy: 0.9984\n",
            "Epoch 5/30\n",
            "1750/1750 [==============================] - 7s 4ms/step - loss: 0.0022 - accuracy: 0.9992 - val_loss: 0.0078 - val_accuracy: 0.9981\n",
            "12463 2568 1881 13088\n",
            "pod:  0.8743403032934732\n",
            "pof:  0.17084691637282948\n",
            "AUC:  0.8517466934603218\n",
            "recall:  0.8743403032934732\n",
            "precision:  0.8359734287174246\n",
            "F1-Score:  0.854726530612245\n",
            "accuracy:  0.8517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RNN\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout, LSTM, GRU\n",
        "from keras.layers import Embedding\n",
        "max_features = 10000 # number of words to consider as features\n",
        "import numpy as np\n",
        "#create model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['acc'])\n",
        "train_en_x=X_encoded.copy(deep=True)\n",
        "test_en_y=X_encoded_test.copy(deep=True)\n",
        "train_en_x[train_en_x < 0]=0\n",
        "test_en_y[test_en_y < 0]=0\n",
        "model_history = model.fit(train_en_x, y_syn_df, epochs=5, batch_size=128, validation_split=0.2)\n",
        "\n",
        "y2_pred = model.predict(test_en_y)\n",
        "y22_pred=y2_pred.round()\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "tn, fp, fn, tp  = confusion_matrix(y_test, y22_pred).ravel()\n",
        "print(tn, fp, fn, tp)\n",
        "pod=tp/(tp+fn)\n",
        "\n",
        "print('pod: ',pod)\n",
        "pof=fp/(fp+tn)\n",
        "print ('pof: ',pof)\n",
        "auc_val=(1+pod-pof)/2\n",
        "print ('AUC: ',auc_val)\n",
        "\n",
        "\n",
        "recall = tp/(tp+fn)\n",
        "print ('recall: ',recall)\n",
        "precision=tp/(tp+fp)\n",
        "print ('precision: ',precision)\n",
        "\n",
        "F1_score=(2*precision*recall)/(precision+recall)\n",
        "print ('F1-Score: ',F1_score)\n",
        "\n",
        "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
        "print ('accuracy: ',accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9R2c_xsOW45",
        "outputId": "149ce453-eb98-4fa3-e1d3-4ffde7f50405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "438/438 [==============================] - 37s 78ms/step - loss: 0.4916 - acc: 0.7562 - val_loss: 0.4259 - val_acc: 0.8229\n",
            "Epoch 2/5\n",
            "438/438 [==============================] - 33s 76ms/step - loss: 0.4116 - acc: 0.8184 - val_loss: 0.4016 - val_acc: 0.8229\n",
            "Epoch 3/5\n",
            "438/438 [==============================] - 33s 76ms/step - loss: 0.4072 - acc: 0.8206 - val_loss: 0.4028 - val_acc: 0.8229\n",
            "Epoch 4/5\n",
            "438/438 [==============================] - 33s 76ms/step - loss: 0.4039 - acc: 0.8226 - val_loss: 0.4167 - val_acc: 0.8229\n",
            "Epoch 5/5\n",
            "438/438 [==============================] - 34s 77ms/step - loss: 0.4048 - acc: 0.8194 - val_loss: 0.4019 - val_acc: 0.8229\n",
            "11103 3928 3736 11233\n",
            "pod:  0.750417529561093\n",
            "pof:  0.261326591710465\n",
            "AUC:  0.7445454689253139\n",
            "recall:  0.750417529561093\n",
            "precision:  0.7409141877184883\n",
            "F1-Score:  0.7456355791569864\n",
            "accuracy:  0.7445333333333334\n"
          ]
        }
      ]
    }
  ]
}